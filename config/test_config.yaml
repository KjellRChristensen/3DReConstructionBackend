# Test Configuration for Quick Convergence Testing
#
# This is a minimal config for smoke testing the training pipeline
# Use with a small dataset (1K-10K samples) to verify everything works
#
# Usage:
#   python -m training.finetune --config config/test_config.yaml

# ==============================================================================
# Model Configuration
# ==============================================================================
base_model: "Yuan-Che/OpenECADv2-SigLIP-0.89B"
model_type: "openecad"

# ==============================================================================
# Data Paths
# ==============================================================================
# Generate test dataset first:
#   python scripts/download_openecad_dataset.py --subset 1000 --output data/training/openecad_test
train_data: "data/training/openecad_test/train/train.json"
val_data: "data/training/openecad_test/val/val.json"
image_folder: "data/training/openecad_test/images"

# ==============================================================================
# Output
# ==============================================================================
output_dir: "checkpoints"
run_name: "test_smoke"

# ==============================================================================
# LoRA Configuration (Minimal for testing)
# ==============================================================================
use_lora: true
lora:
  r: 64            # Smaller rank for faster testing
  alpha: 128
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# ==============================================================================
# Training Hyperparameters (Fast testing config)
# ==============================================================================
training:
  num_epochs: 1              # Just 1 epoch for smoke test
  per_device_batch_size: 2
  gradient_accumulation_steps: 1
  learning_rate: 0.0001
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  weight_decay: 0.0
  max_grad_norm: 1.0

  # Precision
  fp16: true
  bf16: false

  # Memory optimization
  gradient_checkpointing: true

  # Data loading
  dataloader_num_workers: 2

  # Logging & Saving (more frequent for testing)
  logging_steps: 5
  save_steps: 100
  eval_steps: 100
  save_total_limit: 2

# ==============================================================================
# Hardware
# ==============================================================================
device: "auto"
deepspeed_config: null

# ==============================================================================
# Conversation Mode
# ==============================================================================
conv_mode: "phi"
